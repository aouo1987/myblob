[TOC]

---

### _历史服务器_

#### _历史服务器的配置_

```xml
配置文件mapred-site.xml 

	<property>
		<name>mapreduce.jobhistory.address</name>
   	 	<value>master.ssgao:10020</value>
        <description>数据通信的地址</description>
	</property>
	<property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>master.ssgao:19888</value>
        <description>web页面的地址</description>
	</property>

启动历史服务器的命令
mr-jobhistory-daemon.sh start historyserver

```



#### _配置日志聚集_

```xml
配置yarn-site.xml
	<property>
		<name>yarn.log-aggregation-enable</name>
        <value>true</value>
        <description>日志聚集功能打开</description>
	</property>
	<property>
		<name>yarn.log-aggration.retain-seconds</name>
        <value>604800</value>
        <description>日志保留时间设置为7天</description>
	</property>
```



### _编译hadoop_

```shell


[root@slavea apache-ant-1.9.9]# yum install -y  glibc-headers
[root@slavea apache-ant-1.9.9]# yum install -y  gcc-c++
[root@slavea apache-ant-1.9.9]# yum install -y  make cmake


protobuf的编译安装
[root@slavea protobuf-3.0.0]# ./configure
[root@slavea protobuf-3.0.0]# make
[root@slavea protobuf-3.0.0]# make check
[root@slavea protobuf-3.0.0]# make install
[root@slavea protobuf-3.0.0]# ldconfig
[root@slavea protobuf-3.0.0]# vim /etc/profile
	export LD_LIBRARY_PATH=/root/package/protobuf-3.0.0
	export PATH=PATH:$LD_LIBRARY_PATH
[root@slavea protobuf-3.0.0]# protoc --version
	libprotoc 3.0.0

卸载protoc
	[root@slavea ~]# which protoc
	/usr/local/bin/protoc
	[root@slavea ~]# rm /usr/local/bin/protoc 
	rm：是否删除普通文件 "/usr/local/bin/protoc"？y

解压hadoop源码,执行mvn 命令
	[root@slavea hadoop-2.7.7-src]# mvn package -Pdist.native -DskipTests -Dtar
```







```xml
dataNode3 异常,返回nameNode的时候副本数少了一个, NameNode会异步的复制一份文件

dataNode1 异常,首先会重试连接,如果还是连接不上,重新请求nameNode,重新分配dataNode节点
```



### _HDFS的深入_

#### _namenode的工作机制_

```java
Fsimage: 
	namenode中存储的元数据信息进行序列化以后形成的文件
	fsimage0000000001 (n个零)
edits: 记录对namenode中元数据更新的每一步操作
	edits000000001~edits000000005


请求是否需要checkpoint
	checkpoint触发条件 定时时间到   edits 的数据满了
	
第一阶段 NameNode启动
	1) 第一次启动NameNode格式化后,创建fsimage和edits文件.如果不是第一次启动,直接加载编辑日志和镜像文件到内容
	2) 客户端对元数据进行增删改的请求
	3) NameNode记录操作日志,更新滚动日志
	4) NameNode在内存中对数据进行增删改查
第二阶段 Secondary NameNode工作
	1) Secondary NameNode询问NameNode是否需要checkPoint.直接带回NameNode是否需要检查结果
	2) Secondary NameNode 请求执行checkPoint
	3) NameNode 滚动正在写的edits日志
	4) 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode
	5) Secondary NameNode加载编辑日志和镜像文件到内存,并合并
	6) 生成新的镜像文件 fsimage.chkpoint
	7) 拷贝fsimage.chkpoint到NameNode
	8) NameNode将fsiamge.chkpoint重新命名成fsimage

NN和2NN工作机制
Fsimage namenode内存中元数据序列化后形成的文件
Edits 记录客户端更新元数据信息的每一步操作(可通过Edits运算出元数据)
namenode启动时, 先滚动edits并生成一个空的edits.inprogress,然后加载edits和fsimage到内存中,此时nameNode内存就持有最新的元数据信息。client开始对nameNode发送元数据的增删改查的请求,这些请求的操作首先会被记录到edits.inprogress中(查询元数据的操作不会被记录在edits中,因为查询操作不会更改元数据信息).如果此时
namenode挂掉,重启后会从edits中读取元数据的信息。然后,namenode会在内存中执行元数据的增删改查的操作。
	由于edits中记录的操作会越来越多,edits文件会越来越大,导致namenode在启动加载edits时会很慢,所以需要对edits和fsimage进行合并(所谓的合并,就是将edits和fsimage加载到内存中,按照edits的操作一步步执行,最终形成新的fsimage)。SecondaryNameNode的作用就是帮助namenode进行edits和fsimage的合并工作。
	secondaryNamde首先会先询问namenode是否需要checkpoint(触发checkpoint需要满足两个条件中任意一个,定时时间到了和edits中数据写满了)。直接带会namenode是否检查结果.secondarynamenode执行checkpoint操作,首先会让namenode滚动edits并生成一个空的edits.inprogress,滚动edits的目的是给edits打个标记,所以所有新的操作都写如edits.inprogress,其他未合并的edits和fsimage会拷贝到secondarynamenode的本地,然后将拷贝的edits和fsimage加载到内存中进行合并,生成fsimage.chkpoint,然后将fsimage.chkpoint拷贝给namenode,重命名为fsimage后替换掉原来的fsimage。namenode在启动时就只需要加载之前未合并的edits和fsimage即可,因为合并过的edits中元数据信息已经被记录在fsimage中。
	
```



#### _fsimage和edits介绍_

```shell



查看fsimage和edits文件的位置
[root@master current]# 
[root@master current]# pwd
/root/mydata/hadoopname/current
[root@master current]# ls -l
总用量 4168
-rw-r--r--. 1 root root 1048576 12月 19 01:12 edits_0000000000000000001-0000000000000000008
-rw-r--r--. 1 root root      42 12月 20 02:45 edits_0000000000000000009-0000000000000000010
-rw-r--r--. 1 root root      42 12月 20 03:45 edits_0000000000000000011-0000000000000000012
-rw-r--r--. 1 root root 1048576 12月 20 03:45 edits_0000000000000000013-0000000000000000013
-rw-r--r--. 1 root root     728 12月 23 05:30 edits_0000000000000000014-0000000000000000025
-rw-r--r--. 1 root root     128 12月 23 06:30 edits_0000000000000000026-0000000000000000028
-rw-r--r--. 1 root root      42 12月 23 07:30 edits_0000000000000000029-0000000000000000030
-rw-r--r--. 1 root root      42 12月 23 08:30 edits_0000000000000000031-0000000000000000032
-rw-r--r--. 1 root root      42 12月 23 09:30 edits_0000000000000000033-0000000000000000034
-rw-r--r--. 1 root root 1048576 12月 23 09:30 edits_0000000000000000035-0000000000000000035
-rw-r--r--. 1 root root     820 12月 23 13:55 edits_0000000000000000036-0000000000000000049
-rw-r--r--. 1 root root      42 12月 23 14:55 edits_0000000000000000050-0000000000000000051
-rw-r--r--. 1 root root      42 12月 23 15:55 edits_0000000000000000052-0000000000000000053
-rw-r--r--. 1 root root      42 12月 23 16:55 edits_0000000000000000054-0000000000000000055
-rw-r--r--. 1 root root      42 12月 23 17:55 edits_0000000000000000056-0000000000000000057
以上是已经滚动过的edits文件
-rw-r--r--. 1 root root 1048576 12月 23 17:55 edits_inprogress_0000000000000000058 (正在写的edits文件)
-rw-r--r--. 1 root root     856 12月 23 16:55 fsimage_0000000000000000055 镜像
-rw-r--r--. 1 root root      62 12月 23 16:55 fsimage_0000000000000000055.md5 校验
-rw-r--r--. 1 root root     856 12月 23 17:55 fsimage_0000000000000000057
-rw-r--r--. 1 root root      62 12月 23 17:55 fsimage_0000000000000000057.md5
-rw-r--r--. 1 root root       3 12月 23 17:55 seen_txid
-rw-r--r--. 1 root root     206 12月 23 13:02 VERSION

[root@master current]# cat VERSION 
#Sun Dec 23 13:02:41 CST 2018
namespaceID=2083891638
clusterID=CID-44142980-7730-476f-b6af-3b3cfae7c504
cTime=0
storageType=NAME_NODE
blockpoolID=BP-1099844779-192.168.0.108-1545152854774
layoutVersion=-63


[root@master current]# cat seen_txid 
58  --存在的是当前正在写的edits的序号
```







### _网络拓扑概念_

```
在本地网络中,两个节点被称为"彼此近邻"是什么意思
  在海量数据处理中,其主要限制因素是节点之间数据的传输速率——带宽根稀缺,两个节点之间的带宽作为距离的衡量标准.
  节点距离: 两个节点到达最近的共同祖先的距离总和
```



#### _hdfs的机架感知_

```
低版本的hadoop副本节点选择
	第一个副本在client所处的节点上,如果客户端在集群外,随机选择一个
	第二个副本和第一个副本位于不同机架的随机节点上
	第三个副本和第二副本位于相同的机架,节点随机
	

高版本的副本节点选择
	第一个副本在client所处的节点上,如果客户端在集群外,随机选择一个
	第二个副本和第一个副本位于相同的机架,随机节点
	第三个副本位于不同机架,随机节点
```































